{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alanj\\Documents\\GitHub\\dynolearn\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "hub = HuggingFaceHub(repo_id=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_template = \"\"\"Only provide the list\n",
    "\n",
    "{about}. Make a list of 5 suitable tasks to {teaching_task}.\n",
    "\n",
    "Just list out each item 1 by 1 as a JSON list.\"\"\"\n",
    "\n",
    "tasks_prompt = PromptTemplate(\n",
    "    template = tasks_template,\n",
    "    input_variables=[\"about\", \"teaching_task\"]\n",
    ")\n",
    "\n",
    "dialogue_template = \"\"\"About: {about}\n",
    "\n",
    "Question: What dialogue would be helpful in a game where the tasks are:\n",
    "\n",
    "{tasks}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "dialogue_prompt = PromptTemplate(\n",
    "    template = dialogue_template, \n",
    "    input_variables=[\"about\", \"tasks\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_chain = tasks_prompt | hub | hub.bind(stop=\"\\n6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ]"
     ]
    }
   ],
   "source": [
    "for chunk in tasks_chain.stream(    {\n",
    "        \"about\": \"James likes Paw Patrol\", \n",
    "        \"teaching_task\": \"teach james how to get ready for school\"\n",
    "    }):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tasks': ' the bathroom immediately and do his thing.',\n",
       " 'about': StringPromptValue(text='James likes Paw Patrol'),\n",
       " 'dialogue': ' Firstly, let\\'s divide the task into smaller parts. The task is \"the bathroom immediately and do his thing.\" This implies James has to move quickly to the bathroom and perform his tasks in time. He might be late for some reason, or he is in a hurry. Based on this, we can construct a dialogue.\\n\\nJames (nervously): C\\'mon, I need to get to the bathroom immediately!\\nPlayer: Okay, let\\'s hurry up.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_chain = tasks_prompt | hub  | hub.bind(stop=\"\\n6\")\n",
    "dialogue_chain = dialogue_prompt | hub \n",
    "\n",
    "chain = (\n",
    "    {\"tasks\": tasks_chain, \"about\": PromptTemplate.from_template(\"James likes Paw Patrol\")} | RunnablePassthrough.assign(dialogue=dialogue_chain)\n",
    ")\n",
    "\n",
    "chain.stream(\n",
    "    {\n",
    "        \"about\": \"James likes Paw Patrol\", \n",
    "        \"teaching_task\": \"teach james how to get ready for school\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
