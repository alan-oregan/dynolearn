{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade accelerate\n",
    "# !pip install torch --upgrade --index-url https://download.pytorch.org/whl/cu118 # if cuda not working, run as admin\n",
    "# !pip install --upgrade huggingface_hub\n",
    "# !pip install langchain\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = 'stabilityai/stablelm-3b-4e1t'\n",
    "token=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91015570343940c5b11da481ddf4de3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_stablelm_epoch.py:   0%|          | 0.00/5.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stablelm-3b-4e1t:\n",
      "- configuration_stablelm_epoch.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06e9887c5b543f8a88773281b9738b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_stablelm_epoch.py:   0%|          | 0.00/38.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/stabilityai/stablelm-3b-4e1t:\n",
      "- modeling_stablelm_epoch.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableLMEpochForCausalLM(\n",
       "  (model): StableLMEpochModel(\n",
       "    (embed_tokens): Embedding(50304, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  model_id, \n",
    "  trust_remote_code=True,\n",
    "  torch_dtype=\"auto\",\n",
    "  token=token\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  trust_remote_code=True,\n",
    "  torch_dtype=\"auto\",\n",
    "  token=token\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512, # maximum number of new tokens to be generated\n",
    "        temperature=0.5, # randomness 0.0 to 1.0\n",
    "        do_sample=True, # include current generated text in text generation \n",
    "        trust_remote_code=True, # allows external code from model author to run\n",
    "        device=0,  # Use GPU\n",
    "        batch_size=2, # Number of inputs to process at once\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        return_full_text=True,  # langchain expects the full text\n",
    "        repetition_penalty=1.1  # to avoid repitition\n",
    "    )\n",
    ")\n",
    "\n",
    "local_llm.pipeline.tokenizer.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_template = \"\"\"About: {about}\n",
    "\n",
    "Question: Make a list of 5 suitable tasks to \"{teaching_task}\".\n",
    "\n",
    "Answer: Let's just list out each item 1 by 1.\"\"\"\n",
    "\n",
    "tasks_prompt = PromptTemplate(\n",
    "    template = tasks_template,\n",
    "    input_variables=[\"about\", \"teaching_task\"]\n",
    ")\n",
    "\n",
    "dialogue_template = \"\"\"About: {about}\n",
    "\n",
    "Question: What dialogue would be helpful in a game where the tasks are:\n",
    "\n",
    "{tasks}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "dialogue_prompt = PromptTemplate(\n",
    "    template = dialogue_template, \n",
    "    input_variables=[\"about\", \"tasks\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alanj\\Documents\\GitHub\\dynolearn\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tasks': '',\n",
       " 'about': StringPromptValue(text='James likes Paw Patrol'),\n",
       " 'dialogue': \"\\n1) James is an adult, so he has to say 'I like Paw Patrol' instead of 'I love Paw Patrol'.\\n2) He doesn't have to say that at all if there is no way for him to know whether his kid is male or female (or even if it matters).\\n3) If you want to make sure, then you could check if the user's age is less than 18 and add another option 'I don't really care about this question', but I think that will not help much.\\n\\n\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_chain = tasks_prompt | local_llm  | local_llm.bind(stop=\"\\n6\")\n",
    "dialogue_chain = dialogue_prompt | local_llm \n",
    "\n",
    "chain = (\n",
    "    {\"tasks\": tasks_chain, \"about\": PromptTemplate.from_template(\"James likes Paw Patrol\")} | RunnablePassthrough.assign(dialogue=dialogue_chain)\n",
    ")\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"about\": \"James likes Paw Patrol\", \n",
    "        \"teaching_task\": \"teach james how to get ready for school\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
